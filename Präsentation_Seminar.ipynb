{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucascodinglab/Probabilistic_Random_Forest/blob/main/Pr%C3%A4sentation_Seminar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "Sj9eZkbYUeix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuHLNIP7jV_h",
        "outputId": "b2dcdf8a-e8fd-4d14-acca-85953f1e58b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.linear_model\n",
        "import os\n",
        "import urllib\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.linear_model\n",
        "import os\n",
        "import urllib\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import math\n",
        "import warnings\n",
        "from sklearn.impute import SimpleImputer\n",
        "from tqdm import tqdm\n",
        "from numpy.core.numeric import count_nonzero\n",
        "from sklearn.model_selection import train_test_split\n",
        "from heapq import nsmallest\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/P_Seminar/2_heart_dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0bxxpRlq6Q1F",
        "outputId": "bd35f660-8ff0-4966-d00b-9a431cbe5e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  HeartDisease    BMI Smoking AlcoholDrinking Stroke  PhysicalHealth  \\\n",
              "0           No  16.60     Yes              No     No             3.0   \n",
              "1           No  20.34      No              No    Yes             0.0   \n",
              "2           No  26.58     Yes              No     No            20.0   \n",
              "3           No  24.21      No              No     No             0.0   \n",
              "4           No  23.71      No              No     No            28.0   \n",
              "\n",
              "   MentalHealth DiffWalking     Sex  AgeCategory   Race Diabetic  \\\n",
              "0          30.0          No  Female        55-59  White      Yes   \n",
              "1           0.0          No  Female  80 or older  White       No   \n",
              "2          30.0          No    Male        65-69  White      Yes   \n",
              "3           0.0          No  Female        75-79  White       No   \n",
              "4           0.0         Yes  Female        40-44  White       No   \n",
              "\n",
              "  PhysicalActivity  GenHealth  SleepTime Asthma KidneyDisease SkinCancer  \n",
              "0              Yes  Very good        5.0    Yes            No        Yes  \n",
              "1              Yes  Very good        7.0     No            No         No  \n",
              "2              Yes       Fair        8.0    Yes            No         No  \n",
              "3               No       Good        6.0     No            No        Yes  \n",
              "4              Yes  Very good        8.0     No            No         No  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b784c2f4-6ac6-44d8-91d0-04b475cc371a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeartDisease</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoking</th>\n",
              "      <th>AlcoholDrinking</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>PhysicalHealth</th>\n",
              "      <th>MentalHealth</th>\n",
              "      <th>DiffWalking</th>\n",
              "      <th>Sex</th>\n",
              "      <th>AgeCategory</th>\n",
              "      <th>Race</th>\n",
              "      <th>Diabetic</th>\n",
              "      <th>PhysicalActivity</th>\n",
              "      <th>GenHealth</th>\n",
              "      <th>SleepTime</th>\n",
              "      <th>Asthma</th>\n",
              "      <th>KidneyDisease</th>\n",
              "      <th>SkinCancer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No</td>\n",
              "      <td>16.60</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>55-59</td>\n",
              "      <td>White</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No</td>\n",
              "      <td>20.34</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>80 or older</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>7.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No</td>\n",
              "      <td>26.58</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>20.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Male</td>\n",
              "      <td>65-69</td>\n",
              "      <td>White</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Fair</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No</td>\n",
              "      <td>24.21</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>75-79</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Good</td>\n",
              "      <td>6.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No</td>\n",
              "      <td>23.71</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Female</td>\n",
              "      <td>40-44</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b784c2f4-6ac6-44d8-91d0-04b475cc371a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b784c2f4-6ac6-44d8-91d0-04b475cc371a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b784c2f4-6ac6-44d8-91d0-04b475cc371a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"No\",\n{\n            'v': 16.6,\n            'f': \"16.6\",\n        },\n\"Yes\",\n\"No\",\n\"No\",\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 30.0,\n            'f': \"30.0\",\n        },\n\"No\",\n\"Female\",\n\"55-59\",\n\"White\",\n\"Yes\",\n\"Yes\",\n\"Very good\",\n{\n            'v': 5.0,\n            'f': \"5.0\",\n        },\n\"Yes\",\n\"No\",\n\"Yes\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"No\",\n{\n            'v': 20.34,\n            'f': \"20.34\",\n        },\n\"No\",\n\"No\",\n\"Yes\",\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n\"No\",\n\"Female\",\n\"80 or older\",\n\"White\",\n\"No\",\n\"Yes\",\n\"Very good\",\n{\n            'v': 7.0,\n            'f': \"7.0\",\n        },\n\"No\",\n\"No\",\n\"No\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"No\",\n{\n            'v': 26.58,\n            'f': \"26.58\",\n        },\n\"Yes\",\n\"No\",\n\"No\",\n{\n            'v': 20.0,\n            'f': \"20.0\",\n        },\n{\n            'v': 30.0,\n            'f': \"30.0\",\n        },\n\"No\",\n\"Male\",\n\"65-69\",\n\"White\",\n\"Yes\",\n\"Yes\",\n\"Fair\",\n{\n            'v': 8.0,\n            'f': \"8.0\",\n        },\n\"Yes\",\n\"No\",\n\"No\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"No\",\n{\n            'v': 24.21,\n            'f': \"24.21\",\n        },\n\"No\",\n\"No\",\n\"No\",\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n\"No\",\n\"Female\",\n\"75-79\",\n\"White\",\n\"No\",\n\"No\",\n\"Good\",\n{\n            'v': 6.0,\n            'f': \"6.0\",\n        },\n\"No\",\n\"No\",\n\"Yes\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"No\",\n{\n            'v': 23.71,\n            'f': \"23.71\",\n        },\n\"No\",\n\"No\",\n\"No\",\n{\n            'v': 28.0,\n            'f': \"28.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n\"Yes\",\n\"Female\",\n\"40-44\",\n\"White\",\n\"No\",\n\"Yes\",\n\"Very good\",\n{\n            'v': 8.0,\n            'f': \"8.0\",\n        },\n\"No\",\n\"No\",\n\"No\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"HeartDisease\"], [\"number\", \"BMI\"], [\"string\", \"Smoking\"], [\"string\", \"AlcoholDrinking\"], [\"string\", \"Stroke\"], [\"number\", \"PhysicalHealth\"], [\"number\", \"MentalHealth\"], [\"string\", \"DiffWalking\"], [\"string\", \"Sex\"], [\"string\", \"AgeCategory\"], [\"string\", \"Race\"], [\"string\", \"Diabetic\"], [\"string\", \"PhysicalActivity\"], [\"string\", \"GenHealth\"], [\"number\", \"SleepTime\"], [\"string\", \"Asthma\"], [\"string\", \"KidneyDisease\"], [\"string\", \"SkinCancer\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 Percent missing\n",
        "Thomas = {'HeartDisease':np.nan, 'BMI_cat':2, 'Smoking':0, 'AlcoholDrinking':np.nan, 'Stroke':0,\n",
        "       'PhysicalHealth_cat':0, 'MentalHealth_cat':0, 'DiffWalking':np.nan, 'Sex':0, 'AgeCategory_cat':0,\n",
        "       'Race_cat': np.nan, 'Diabetic_cat': 0, 'PhysicalActivity_cat':np.nan, 'GenHealth_cat':0, 'SleepTime_cat':1,\n",
        "       'Asthma': 0, 'KidneyDisease':np.nan, 'SkinCancer':0}\n",
        "# 50 Percent missing\n",
        "Alina = {'HeartDisease':np.nan, 'BMI_cat':1, 'Smoking':np.nan, 'AlcoholDrinking':0, 'Stroke':0,\n",
        "       'PhysicalHealth_cat':0, 'MentalHealth_cat':np.nan, 'DiffWalking':np.nan, 'Sex':1, 'AgeCategory_cat': np.nan,\n",
        "       'Race_cat': np.nan, 'Diabetic_cat': np.nan, 'PhysicalActivity_cat':1, 'GenHealth_cat':0, 'SleepTime_cat':2,\n",
        "       'Asthma': np.nan, 'KidneyDisease':np.nan, 'SkinCancer':0}\n",
        "# 60 Percent missing\n",
        "Lucas = {'HeartDisease':np.nan, 'BMI_cat':1, 'Smoking':0, 'AlcoholDrinking':np.nan, 'Stroke':0,\n",
        "       'PhysicalHealth_cat':0, 'MentalHealth_cat':0, 'DiffWalking':np.nan, 'Sex':0, 'AgeCategory_cat': np.nan,\n",
        "       'Race_cat': np.nan, 'Diabetic_cat': 0, 'PhysicalActivity_cat':np.nan, 'GenHealth_cat':np.nan, 'SleepTime_cat':np.nan,\n",
        "       'Asthma': 0, 'KidneyDisease':np.nan, 'SkinCancer':np.nan}"
      ],
      "metadata": {
        "id": "oByJ_Y8m3mtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Pg0Supulw-"
      },
      "source": [
        "# Import Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFJcFMgzukhp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#shorten the dataset and clear out class inbalance\n",
        "\n",
        "df['HeartDisease'] = df['HeartDisease'].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "\n",
        "df_grby = df.groupby(\"HeartDisease\")\n",
        "df0 = df_grby.get_group(0) \n",
        "df1 = df_grby.get_group(1)\n",
        "\n",
        "\n",
        "df0_bt = df0.sample(n = 300, replace = False,random_state = 31) #pick randomly given number of instances from dataframe without replacement\n",
        "df1_bt = df1.sample(n = 300, replace = False, random_state = 31) \n",
        "\n",
        "data_set = pd.concat([df0_bt,df1_bt],ignore_index = True) #join to one dataframe\n",
        "df = data_set.sample(frac=1,random_state = 31).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Smoking\"] = df[\"Smoking\"].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "df['AlcoholDrinking'] = df['AlcoholDrinking'].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "df[\"Stroke\"] = df[\"Stroke\"].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "df[\"DiffWalking\"] = df[\"DiffWalking\"].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "df[\"Sex\"] = df[\"Sex\"].apply(lambda x: 0 if x.strip()=='Male' else 1)\n",
        "df[\"Asthma\"] = df[\"Asthma\"].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "df[\"KidneyDisease\"] = df[\"KidneyDisease\"].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "df[\"SkinCancer\"] = df[\"SkinCancer\"].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "df[\"PhysicalActivity\"] = df[\"PhysicalActivity\"].apply(lambda x: 0 if x.strip()=='No' else 1)\n",
        "\n",
        "one_hot_encoded_data = df.copy()\n",
        "# Creating a instance of label Encoder.\n",
        "le = LabelEncoder()\n",
        "\n",
        "label = le.fit_transform(df[\"Race\"])\n",
        "one_hot_encoded_data[\"Race\"] = label\n",
        "one_hot_encoded_data.rename(columns={\"Race\":\"Race_cat\"}, inplace = True)\n",
        "\n",
        "label = le.fit_transform(df[\"Diabetic\"])\n",
        "one_hot_encoded_data[\"Diabetic\"] = label\n",
        "one_hot_encoded_data.rename(columns={\"Diabetic\":\"Diabetic_cat\"}, inplace = True)\n",
        "\n",
        "label = le.fit_transform(df[\"GenHealth\"])\n",
        "one_hot_encoded_data[\"GenHealth\"] = label\n",
        "one_hot_encoded_data.rename(columns={\"GenHealth\":\"GenHealth_cat\"}, inplace = True)\n",
        "\n",
        "# Using .fit_transform function to fit label\n",
        "# encoder and return encoded label\n",
        "label = le.fit_transform(df['AgeCategory'])\n",
        "one_hot_encoded_data[\"AgeCategory\"] = label\n",
        "one_hot_encoded_data[\"AgeCategory\"] = pd.cut(one_hot_encoded_data['AgeCategory'], bins=[0, 3, 5, 8, 10, 13])\n",
        "one_hot_encoded_data.rename(columns={\"AgeCategory\":\"AgeCategory_cat\"}, inplace = True)\n",
        "label = le.fit_transform(one_hot_encoded_data['AgeCategory_cat'])\n",
        "one_hot_encoded_data[\"AgeCategory_cat\"]=label\n",
        "\n",
        "#Split BMI numerical Data into categorial Data by the official BMI Categories\n",
        "one_hot_encoded_data[\"BMI\"] = pd.cut(one_hot_encoded_data['BMI'], bins=[0, 20, 23, 27, 32, 36, 99])\n",
        "one_hot_encoded_data.rename(columns={\"BMI\":\"BMI_cat\"}, inplace = True)\n",
        "label = le.fit_transform(one_hot_encoded_data['BMI_cat'])\n",
        "one_hot_encoded_data[\"BMI_cat\"]=label\n",
        "\n",
        "#Split PhysicalHealth numerical Data into categorial Data by the official PhysicalHealth Categories\n",
        "one_hot_encoded_data[\"PhysicalHealth\"] = pd.cut(one_hot_encoded_data['PhysicalHealth'], bins=[0,5,10,31])\n",
        "one_hot_encoded_data.rename(columns={\"PhysicalHealth\":\"PhysicalHealth_cat\"}, inplace = True)\n",
        "label = le.fit_transform(one_hot_encoded_data['PhysicalHealth_cat'])\n",
        "one_hot_encoded_data[\"PhysicalHealth_cat\"]=label\n",
        "\n",
        "#Split MentalHealth numerical Data into categorial Data by the official MentalHealth Categories\n",
        "one_hot_encoded_data[\"MentalHealth\"] = pd.cut(one_hot_encoded_data['MentalHealth'], bins=[0,5,10,31])\n",
        "one_hot_encoded_data.rename(columns={\"MentalHealth\":\"MentalHealth_cat\"}, inplace = True)\n",
        "label = le.fit_transform(one_hot_encoded_data['MentalHealth_cat'])\n",
        "one_hot_encoded_data[\"MentalHealth_cat\"]=label\n",
        "\n",
        "#Split SleepTime numerical Data into categorial Data by the official SleepTime Categories\n",
        "one_hot_encoded_data[\"SleepTime\"] = pd.cut(one_hot_encoded_data['SleepTime'], bins=[0,5,7,9,10,25])\n",
        "one_hot_encoded_data.rename(columns={\"SleepTime\":\"SleepTime_cat\"}, inplace = True)\n",
        "label = le.fit_transform(one_hot_encoded_data['SleepTime_cat'])\n",
        "one_hot_encoded_data[\"SleepTime_cat\"]=label\n",
        "\n"
      ],
      "metadata": {
        "id": "u0COM93i6VNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "one_hot_encoded_data = one_hot_encoded_data.append(Thomas, ignore_index = True)\n",
        "one_hot_encoded_data = one_hot_encoded_data.append(Alina, ignore_index = True)\n",
        "one_hot_encoded_data = one_hot_encoded_data.append(Lucas, ignore_index = True)"
      ],
      "metadata": {
        "id": "-L9YG6OWAPEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Probabilities - KNN Algorithm"
      ],
      "metadata": {
        "id": "uZFkoTpI3D70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#randomly delete 20 percent of data and replace it with NaN-values\n",
        "#delete Diabetes_012 column\n",
        "data_delete = one_hot_encoded_data.drop(columns='HeartDisease')\n",
        "# get dimensions of data_delete\n",
        "n_rows, n_cols = len(data_delete.index), len(data_delete.columns)       \n",
        "volume = n_rows * n_cols         \n",
        "volume_nan = int(volume * 0.5)      \n",
        "\n",
        "# create random index locations for NaNs\n",
        "index = np.random.seed(32)\n",
        "index = np.random.randint(volume, size=volume_nan)\n",
        "row_index = index % n_rows\n",
        "col_index = (index / n_rows).astype(int)\n",
        "\n",
        "\n",
        "# assign NaN to each of the indices in data_delete\n",
        "for ri, ci in zip(row_index, col_index):\n",
        "  data_delete.iloc[ri, ci] = np.nan\n",
        "\n",
        "data_delete.head(10)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v__OzLXqfOyq",
        "outputId": "24db7997-936a-42ba-e42b-36b30c045e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   BMI_cat  Smoking  AlcoholDrinking  Stroke  PhysicalHealth_cat  \\\n",
              "0      NaN      1.0              0.0     NaN                 NaN   \n",
              "1      NaN      1.0              NaN     NaN                 3.0   \n",
              "2      NaN      NaN              NaN     0.0                 NaN   \n",
              "3      NaN      1.0              0.0     0.0                 2.0   \n",
              "4      NaN      NaN              0.0     NaN                 1.0   \n",
              "5      4.0      0.0              NaN     NaN                 0.0   \n",
              "6      NaN      1.0              NaN     0.0                 NaN   \n",
              "7      NaN      1.0              0.0     NaN                 NaN   \n",
              "8      1.0      0.0              0.0     0.0                 NaN   \n",
              "9      5.0      NaN              1.0     0.0                 NaN   \n",
              "\n",
              "   MentalHealth_cat  DiffWalking  Sex  AgeCategory_cat  Race_cat  \\\n",
              "0               3.0          NaN  NaN              2.0       NaN   \n",
              "1               3.0          0.0  NaN              2.0       NaN   \n",
              "2               3.0          0.0  NaN              NaN       NaN   \n",
              "3               NaN          1.0  NaN              NaN       2.0   \n",
              "4               2.0          NaN  NaN              NaN       NaN   \n",
              "5               NaN          0.0  NaN              NaN       5.0   \n",
              "6               3.0          NaN  1.0              NaN       5.0   \n",
              "7               3.0          0.0  0.0              3.0       5.0   \n",
              "8               3.0          0.0  1.0              NaN       5.0   \n",
              "9               2.0          1.0  0.0              3.0       5.0   \n",
              "\n",
              "   Diabetic_cat  PhysicalActivity  GenHealth_cat  SleepTime_cat  Asthma  \\\n",
              "0           0.0               1.0            4.0            NaN     NaN   \n",
              "1           0.0               1.0            2.0            1.0     0.0   \n",
              "2           NaN               1.0            NaN            2.0     NaN   \n",
              "3           NaN               NaN            1.0            2.0     0.0   \n",
              "4           0.0               NaN            2.0            0.0     NaN   \n",
              "5           1.0               1.0            NaN            NaN     NaN   \n",
              "6           NaN               1.0            1.0            2.0     0.0   \n",
              "7           NaN               NaN            NaN            2.0     NaN   \n",
              "8           NaN               1.0            NaN            1.0     0.0   \n",
              "9           0.0               0.0            NaN            NaN     0.0   \n",
              "\n",
              "   KidneyDisease  SkinCancer  PhysicalActivity_cat  \n",
              "0            0.0         NaN                   NaN  \n",
              "1            NaN         0.0                   NaN  \n",
              "2            0.0         NaN                   NaN  \n",
              "3            NaN         NaN                   NaN  \n",
              "4            0.0         NaN                   NaN  \n",
              "5            0.0         NaN                   NaN  \n",
              "6            1.0         0.0                   NaN  \n",
              "7            NaN         NaN                   NaN  \n",
              "8            0.0         0.0                   NaN  \n",
              "9            0.0         NaN                   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bbbe1430-e806-4653-a2c0-b243f2a2242f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BMI_cat</th>\n",
              "      <th>Smoking</th>\n",
              "      <th>AlcoholDrinking</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>PhysicalHealth_cat</th>\n",
              "      <th>MentalHealth_cat</th>\n",
              "      <th>DiffWalking</th>\n",
              "      <th>Sex</th>\n",
              "      <th>AgeCategory_cat</th>\n",
              "      <th>Race_cat</th>\n",
              "      <th>Diabetic_cat</th>\n",
              "      <th>PhysicalActivity</th>\n",
              "      <th>GenHealth_cat</th>\n",
              "      <th>SleepTime_cat</th>\n",
              "      <th>Asthma</th>\n",
              "      <th>KidneyDisease</th>\n",
              "      <th>SkinCancer</th>\n",
              "      <th>PhysicalActivity_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbbe1430-e806-4653-a2c0-b243f2a2242f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bbbe1430-e806-4653-a2c0-b243f2a2242f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bbbe1430-e806-4653-a2c0-b243f2a2242f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 4.0,\n            'f': \"4.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 4.0,\n            'f': \"4.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 5.0,\n            'f': \"5.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 5.0,\n            'f': \"5.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 5.0,\n            'f': \"5.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 5.0,\n            'f': \"5.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 5.0,\n            'f': \"5.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 2.0,\n            'f': \"2.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 3.0,\n            'f': \"3.0\",\n        },\n{\n            'v': 5.0,\n            'f': \"5.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"BMI_cat\"], [\"number\", \"Smoking\"], [\"number\", \"AlcoholDrinking\"], [\"number\", \"Stroke\"], [\"number\", \"PhysicalHealth_cat\"], [\"number\", \"MentalHealth_cat\"], [\"number\", \"DiffWalking\"], [\"number\", \"Sex\"], [\"number\", \"AgeCategory_cat\"], [\"number\", \"Race_cat\"], [\"number\", \"Diabetic_cat\"], [\"number\", \"PhysicalActivity\"], [\"number\", \"GenHealth_cat\"], [\"number\", \"SleepTime_cat\"], [\"number\", \"Asthma\"], [\"number\", \"KidneyDisease\"], [\"number\", \"SkinCancer\"], [\"number\", \"PhysicalActivity_cat\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1 : Filling the NaN-values with k-nearest neighbor algorithm\n",
        "# For each NaN-value: calculating the k-nearest neighbors + calculating based on that the probability of each occurence of an attribute\n",
        "#from pandas.core import indexing\n",
        "\n",
        "# replace Nan-values with mean\n",
        "df_mean = one_hot_encoded_data.copy()\n",
        "df_mean[\"HeartDisease\"] = one_hot_encoded_data[\"HeartDisease\"]\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "def pre_nan():  \n",
        "  for i in range(0,17):\n",
        "    df_mean.iloc[:,[i]] = mean_imputer.fit_transform(df_mean.iloc[:,[i]].values.reshape(-1,1))\n",
        "pre_nan()\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder()\n",
        "transformed = ohe.fit_transform(df_mean)\n",
        "df_mean = transformed.toarray()\n",
        "\n",
        "array_datamean = df_mean\n",
        "#array_datamean = df_mean.to_numpy()\n",
        "\n",
        "\n",
        "# functions for calculating the k nearest neighbor\n",
        "import random\n",
        "# calculates the euclidean distance between two arrays\n",
        "def euclidean_distance(array1, array2):\n",
        "  distance=0.0\n",
        "  for i in range(len(array1)):\n",
        "      distance = distance + (array1[i]- array2[i])**2\n",
        "      result = math.sqrt(distance)\n",
        "  return result\n",
        "\n",
        "# get nearest neighbors (distances = row values, distance, row index / neighbors = row index of knn)\n",
        "def get_neighbors(train, test_row, k):\n",
        "  #train = train.sample(n=1500,replace=False) #Lucas hinzugefgt\n",
        "  distances = list() \n",
        "  for i, train_row in enumerate(train):\n",
        "    dist = euclidean_distance(test_row, train_row)\n",
        "    distances.append((train_row, dist,i))\n",
        "  distances.sort(key=lambda tup: tup[1])  #sorts the distances in ascending order\n",
        "  neighbors = list()\n",
        "  for i in range(k+1):                          \n",
        "    neighbors.append(distances[i][2])\n",
        "  return neighbors\n",
        "\n",
        "\n",
        "#counts the occurences of each class in the knn for binary attributes\n",
        "def count_occbin (listinput, k, column):\n",
        "  new_valar = []\n",
        "  new_values = []\n",
        "  cluster_0 = 0\n",
        "  cluster_1 = 0\n",
        "  count = 0\n",
        "  for x in listinput: \n",
        "    val = data_delete.iloc[x][column]\n",
        "    if (val == 1.0):\n",
        "      cluster_1 = cluster_1 + 1.0\n",
        "    elif (val == 0.0):\n",
        "      cluster_0 = cluster_0 + 1.0\n",
        "    elif math.isnan(val):             #ignore nan values in calculation\n",
        "      count = count + 1\n",
        "      continue\n",
        "  division = k-count\n",
        "  if division == 0:\n",
        "    division = 0.0000000000000000000001\n",
        "  new_valar = [(cluster_0/division),(cluster_1/division)] \n",
        "  new_values.append(new_valar)\n",
        "  new_values = np.array(new_values)\n",
        "  return new_values\n",
        "\n",
        "#counts the occurences of each class in the knn for categorial attributes\n",
        "def count_occcat (listinput,k,column): \n",
        "  new_valar = []\n",
        "  new_values = []\n",
        "  cluster_0 = 0\n",
        "  cluster_1 = 0\n",
        "  cluster_2 = 0\n",
        "  cluster_3 = 0\n",
        "  cluster_4 = 0\n",
        "  cluster_5 = 0\n",
        "  count = 0\n",
        "  for x in listinput: \n",
        "    val = data_delete.iloc[x][column]\n",
        "    if (val == 0.0):\n",
        "      cluster_0 = cluster_0 + 1.0\n",
        "    elif (val == 1.0):\n",
        "      cluster_1 = cluster_1 + 1.0\n",
        "    elif (val == 2.0):\n",
        "      cluster_2 = cluster_2 + 1.0\n",
        "    elif (val == 3.0):\n",
        "      cluster_3 = cluster_3 + 1.0\n",
        "    elif (val == 4.0):\n",
        "      cluster_4 = cluster_4 + 1.0\n",
        "    elif (val == 5.0):\n",
        "      cluster_5 = cluster_5 + 1.0\n",
        "    elif math.isnan(val):             #ignore nan values in calculation\n",
        "      count = count + 1\n",
        "      continue\n",
        "  division = k-count\n",
        "  if division == 0:\n",
        "    division = 0.0000000000000000000001\n",
        "  new_valar = [(cluster_0/division),(cluster_1/division),(cluster_2/division), (cluster_3/division), (cluster_4/division), (cluster_5/division)]\n",
        "  new_values.append(new_valar)\n",
        "  new_values = np.array(new_values)\n",
        "  return new_values\n",
        "\n",
        "\n",
        "  # gets k nearest neighbors from NaN values and creates new probabilities\n",
        "from tqdm import tqdm\n",
        "# fill new table for binary attributes\n",
        "def filltablebin (x):\n",
        "  newcolumn1 =[]\n",
        "  newcolumn0 = []\n",
        "  for i, row in tqdm(data_delete.iterrows()): \n",
        "    if math.isnan(row[x]) == False:\n",
        "      if row[x] == 1.0:\n",
        "        newcolumn1.append(1.0)\n",
        "        newcolumn0.append(0.0)\n",
        "      elif row[x] == 0.0:\n",
        "           newcolumn1.append(0.0)\n",
        "           newcolumn0.append(1.0)\n",
        "    elif math.isnan(row[x]):                                         \n",
        "         neighbors = get_neighbors(array_datamean, array_datamean[i], 15)  \n",
        "         neighbors.remove(i)   #removes own row index out of neighbors (as it is always distance 0)\n",
        "         prob_nan = count_occbin(neighbors,15,x) #here\n",
        "         one = prob_nan[0,1]\n",
        "         zero = prob_nan[0,0]\n",
        "         newcolumn1.append(one)\n",
        "         newcolumn0.append(zero)\n",
        "  return newcolumn0, newcolumn1\n",
        "\n",
        "# fill new table for categorial attributes\n",
        "def filltablecat (x):\n",
        "  newcolumn0 = []\n",
        "  newcolumn1 =[]\n",
        "  newcolumn2 = []\n",
        "  newcolumn3 = []\n",
        "  newcolumn4 = []\n",
        "  newcolumn5 = []\n",
        "  arr = []\n",
        "  for i, row in tqdm(data_delete.iterrows()): \n",
        "    if math.isnan(row[x]) == False:\n",
        "      if row[x] == 0.0:\n",
        "        newcolumn1.append(0.0)\n",
        "        newcolumn0.append(1.0)\n",
        "        newcolumn2.append(0.0)\n",
        "        newcolumn3.append(0.0)\n",
        "        newcolumn4.append(0.0)\n",
        "        newcolumn5.append(0.0)\n",
        "      elif row[x] == 1.0:\n",
        "        newcolumn1.append(1.0)\n",
        "        newcolumn0.append(0.0)\n",
        "        newcolumn2.append(0.0)\n",
        "        newcolumn3.append(0.0)\n",
        "        newcolumn4.append(0.0)\n",
        "        newcolumn5.append(0.0)\n",
        "      elif row[x] == 2.0:\n",
        "        newcolumn1.append(0.0)\n",
        "        newcolumn0.append(0.0)\n",
        "        newcolumn2.append(1.0)\n",
        "        newcolumn3.append(0.0)\n",
        "        newcolumn4.append(0.0)\n",
        "        newcolumn5.append(0.0)\n",
        "      elif row[x] == 3.0:\n",
        "        newcolumn1.append(0.0)\n",
        "        newcolumn0.append(0.0)\n",
        "        newcolumn2.append(0.0)\n",
        "        newcolumn3.append(1.0)\n",
        "        newcolumn4.append(0.0)\n",
        "        newcolumn5.append(0.0)\n",
        "      elif row[x] == 4.0:\n",
        "        newcolumn1.append(0.0)\n",
        "        newcolumn0.append(0.0)\n",
        "        newcolumn2.append(0.0)\n",
        "        newcolumn3.append(0.0)\n",
        "        newcolumn4.append(1.0)\n",
        "        newcolumn5.append(0.0)\n",
        "      elif row[x] == 5.0:\n",
        "        newcolumn1.append(0.0)\n",
        "        newcolumn0.append(0.0)\n",
        "        newcolumn2.append(0.0)\n",
        "        newcolumn3.append(0.0)\n",
        "        newcolumn4.append(0.0)\n",
        "        newcolumn5.append(1.0)\n",
        "    elif math.isnan(row[x]):                                     \n",
        "         neighbors = get_neighbors(array_datamean, array_datamean[i], 15)   \n",
        "         neighbors.remove(i)   #removes own row index out of neighbors (as it is always distance 0)\n",
        "         prob_nan = count_occcat(neighbors,15,x)\n",
        "         zero = prob_nan[0,0]\n",
        "         one = prob_nan[0,1]\n",
        "         two = prob_nan[0,2]\n",
        "         three = prob_nan[0,3]\n",
        "         four = prob_nan[0,4]\n",
        "         five = prob_nan[0,5]\n",
        "         newcolumn0.append(zero)\n",
        "         newcolumn1.append(one)\n",
        "         newcolumn2.append(two)\n",
        "         newcolumn3.append(three)\n",
        "         newcolumn4.append(four)\n",
        "         newcolumn5.append(five)\n",
        "         arr = newcolumn0, newcolumn1, newcolumn2, newcolumn3, newcolumn4, newcolumn5\n",
        "  return arr\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "F0X7DK5aANgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fill new table with probabilities\n",
        "columns = ['HeartDisease', 'Smoking_No', 'Smoking_Yes', 'AlcoholDrinking_No',\n",
        "       'AlcoholDrinking_Yes', 'DiffWalking_No', 'DiffWalking_Yes',\n",
        "       'Sex_Female', 'Sex_Male', 'Race_American Indian/Alaskan Native',\n",
        "       'Race_Asian', 'Race_Black', 'Race_Hispanic', 'Race_Other', 'Race_White',\n",
        "       'Diabetic_No', 'Diabetic_No, borderline diabetes', 'Diabetic_Yes',\n",
        "       'Diabetic_Yes (during pregnancy)', 'PhysicalActivity_No',\n",
        "       'PhysicalActivity_Yes', 'GenHealth_Excellent', 'GenHealth_Fair',\n",
        "       'GenHealth_Good', 'GenHealth_Poor', 'GenHealth_Very good', 'Asthma_No',\n",
        "       'Asthma_Yes', 'KidneyDisease_No', 'KidneyDisease_Yes', 'SkinCancer_No',\n",
        "       'SkinCancer_Yes', 'Stroke_No', 'Stroke_Yes', 'BMI_cat_0', 'BMI_cat_1',\n",
        "       'BMI_cat_2', 'BMI_cat_3', 'BMI_cat_4', 'BMI_cat_5',\n",
        "       'PhysicalHealth_cat_0', 'PhysicalHealth_cat_1', 'PhysicalHealth_cat_2',\n",
        "       'PhysicalHealth_cat_3', 'MentalHealth_cat_0', 'MentalHealth_cat_1',\n",
        "       'MentalHealth_cat_2', 'MentalHealth_cat_3', 'AgeCategory_cat_0',\n",
        "       'AgeCategory_cat_1', 'AgeCategory_cat_2', 'AgeCategory_cat_3',\n",
        "       'AgeCategory_cat_4', 'AgeCategory_cat_5', 'SleepTime_cat_0',\n",
        "       'SleepTime_cat_1', 'SleepTime_cat_2', 'SleepTime_cat_3',\n",
        "       'SleepTime_cat_4']\n",
        "df_splitknn = pd.DataFrame(columns=columns)\n",
        "newcol_smoking = list()\n",
        "newcol_alcoholdrinking = list()\n",
        "newcol_diffwalking = list()\n",
        "newcol_sex = list()\n",
        "newcol_race = list()\n",
        "newcol_diabetic = list()\n",
        "newcol_physicalactivity = list()\n",
        "newcol_genhealth = list()\n",
        "newcol_asthma = list()\n",
        "newcol_kidneydisease = list()\n",
        "newcol_skincancer = list()\n",
        "newcol_stroke = list()\n",
        "newcol_bmi = list()\n",
        "newcol_physicalhealth = list()\n",
        "newcol_mentalhealth = list()\n",
        "newcol_age = list()\n",
        "newcol_sleeptime = list()\n",
        "\n",
        "df_splitknn['HeartDisease'] = one_hot_encoded_data['HeartDisease']\n",
        "\n",
        "newcol_smoking = filltablebin('Smoking')\n",
        "df_splitknn[\"Smoking_No\"]= newcol_smoking[0]\n",
        "df_splitknn['Smoking_Yes'] = newcol_smoking[1]\n",
        "\n",
        "newcol_alcoholdrinking = filltablebin('AlcoholDrinking')\n",
        "df_splitknn['AlcoholDrinking_No'] = newcol_alcoholdrinking[0]\n",
        "df_splitknn['AlcoholDrinking_Yes'] =newcol_alcoholdrinking[1]\n",
        "\n",
        "newcol_diffwalking = filltablebin('DiffWalking')\n",
        "df_splitknn['DiffWalking_No'] = newcol_diffwalking[0]\n",
        "df_splitknn['DiffWalking_Yes'] =newcol_diffwalking[1]\n",
        "\n",
        "newcol_sex = filltablebin('Sex')\n",
        "df_splitknn['Sex_Male'] = newcol_sex[0]\n",
        "df_splitknn['Sex_Female'] =newcol_sex[1]\n",
        "\n",
        "newcol_race = filltablecat('Race_cat')\n",
        "df_splitknn['Race_American Indian/Alaskan Native'] = newcol_race[0]\n",
        "df_splitknn['Race_Asian'] =newcol_race[1]\n",
        "df_splitknn['Race_Black'] =newcol_race[2]\n",
        "df_splitknn['Race_Hispanic'] =newcol_race[3]\n",
        "df_splitknn['Race_Other'] =newcol_race[4]\n",
        "df_splitknn['Race_White'] =newcol_race[5]\n",
        "\n",
        "\n",
        "newcol_diabetic = filltablecat('Diabetic_cat')\n",
        "df_splitknn['Diabetic_No'] = newcol_diabetic[0]\n",
        "df_splitknn['Diabetic_No, borderline diabetes'] =newcol_diabetic[1]\n",
        "df_splitknn['Diabetic_Yes'] =newcol_diabetic[2]\n",
        "df_splitknn['Diabetic_Yes (during pregnancy)'] =newcol_diabetic[3]\n",
        "\n",
        "\n",
        "newcol_physicalactivity = filltablebin('PhysicalActivity')\n",
        "df_splitknn['PhysicalActivity_No'] = newcol_physicalactivity[0]\n",
        "df_splitknn['PhysicalActivity_Yes'] =newcol_physicalactivity[1]\n",
        "\n",
        "newcol_genhealth = filltablecat('GenHealth_cat')\n",
        "df_splitknn['GenHealth_Excellent'] = newcol_genhealth[0]\n",
        "df_splitknn['GenHealth_Fair'] =newcol_genhealth[1]\n",
        "df_splitknn['GenHealth_Good'] =newcol_genhealth[2]\n",
        "df_splitknn['GenHealth_Poor'] =newcol_genhealth[3]\n",
        "df_splitknn['GenHealth_Very good'] =newcol_genhealth[4]\n",
        "\n",
        "newcol_asthma = filltablebin('Asthma')\n",
        "df_splitknn['Asthma_No'] = newcol_asthma[0]\n",
        "df_splitknn['Asthma_Yes'] =newcol_asthma[1]\n",
        "\n",
        "newcol_kidneydisease = filltablebin('KidneyDisease')\n",
        "df_splitknn['KidneyDisease_No'] = newcol_kidneydisease[0]\n",
        "df_splitknn['KidneyDisease_Yes'] =newcol_kidneydisease[1]\n",
        "\n",
        "newcol_skincancer = filltablebin('SkinCancer')\n",
        "df_splitknn['SkinCancer_No'] = newcol_skincancer[0]\n",
        "df_splitknn['SkinCancer_Yes'] =newcol_skincancer[1]\n",
        "\n",
        "newcol_stroke = filltablebin('Stroke')\n",
        "df_splitknn['Stroke_No'] = newcol_stroke[0]\n",
        "df_splitknn['Stroke_Yes'] =newcol_stroke[1]\n",
        "\n",
        "newcol_bmi = filltablecat('BMI_cat')\n",
        "df_splitknn['BMI_cat_0'] = newcol_bmi[0]\n",
        "df_splitknn['BMI_cat_1'] =newcol_bmi[1]\n",
        "df_splitknn['BMI_cat_2'] =newcol_bmi[2]\n",
        "df_splitknn['BMI_cat_3'] =newcol_bmi[3]\n",
        "df_splitknn['BMI_cat_4'] =newcol_bmi[4]\n",
        "df_splitknn['BMI_cat_5'] =newcol_bmi[5]\n",
        "\n",
        "newcol_physicalhealth = filltablecat('PhysicalHealth_cat')\n",
        "df_splitknn['PhysicalHealth_cat_0'] = newcol_physicalhealth[0]\n",
        "df_splitknn['PhysicalHealth_cat_1'] =newcol_physicalhealth[1]\n",
        "df_splitknn['PhysicalHealth_cat_2'] =newcol_physicalhealth[2]\n",
        "df_splitknn['PhysicalHealth_cat_3'] =newcol_physicalhealth[3]\n",
        "\n",
        "newcol_mentalhealth = filltablecat('MentalHealth_cat')\n",
        "df_splitknn['MentalHealth_cat_0'] = newcol_mentalhealth[0]\n",
        "df_splitknn['MentalHealth_cat_1'] =newcol_mentalhealth[1]\n",
        "df_splitknn['MentalHealth_cat_2'] =newcol_mentalhealth[2]\n",
        "df_splitknn['MentalHealth_cat_3'] =newcol_mentalhealth[3]\n",
        "\n",
        "newcol_age = filltablecat('AgeCategory_cat')\n",
        "df_splitknn['AgeCategory_cat_0'] = newcol_age[0]\n",
        "df_splitknn['AgeCategory_cat_1'] =newcol_age[1]\n",
        "df_splitknn['AgeCategory_cat_2'] =newcol_age[2]\n",
        "df_splitknn['AgeCategory_cat_3'] =newcol_age[3]\n",
        "df_splitknn['AgeCategory_cat_4'] =newcol_age[4]\n",
        "df_splitknn['AgeCategory_cat_5'] =newcol_age[5]\n",
        "\n",
        "newcol_sleeptime = filltablecat('SleepTime_cat')\n",
        "df_splitknn['SleepTime_cat_0'] = newcol_sleeptime[0]\n",
        "df_splitknn['SleepTime_cat_1'] =newcol_sleeptime[1]\n",
        "df_splitknn['SleepTime_cat_2'] =newcol_sleeptime[2]\n",
        "df_splitknn['SleepTime_cat_3'] =newcol_sleeptime[3]\n",
        "df_splitknn['SleepTime_cat_4'] =newcol_sleeptime[4]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jz9aprBc3Cr",
        "outputId": "9a816939-63b0-40b0-c154-8ca671cd8032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "603it [00:11, 51.37it/s]\n",
            "603it [00:10, 57.19it/s]\n",
            "603it [00:13, 44.21it/s]\n",
            "603it [00:10, 57.04it/s]\n",
            "603it [00:09, 61.43it/s]\n",
            "603it [00:11, 54.57it/s]\n",
            "603it [00:10, 56.43it/s]\n",
            "603it [00:08, 72.62it/s]\n",
            "603it [00:10, 55.85it/s]\n",
            "603it [00:10, 58.14it/s]\n",
            "603it [00:07, 77.54it/s]\n",
            "603it [00:12, 48.58it/s]\n",
            "603it [00:11, 51.41it/s]\n",
            "603it [00:10, 59.76it/s]\n",
            "603it [00:08, 68.45it/s]\n",
            "603it [00:10, 57.65it/s]\n",
            "603it [00:10, 55.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7R3pRrs5vKd"
      },
      "source": [
        "## 2. Entropy calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU_GA6jn6u0M"
      },
      "outputs": [],
      "source": [
        "column_dict = {\"Smoking\":[\"Smoking_No\",\"Smoking_Yes\"],\n",
        "               \"AlcoholDrinking\": [\"AlcoholDrinking_No\", \"AlcoholDrinking_Yes\"],\n",
        "               \"DiffWalking\": [\"DiffWalking_No\",\"DiffWalking_Yes\"],\n",
        "               \"Sex\":[\"Sex_Female\",\"Sex_Male\"],\n",
        "               \"Race\":[\"Race_American Indian/Alaskan Native\",\"Race_Asian\",\"Race_Black\",\"Race_Hispanic\",\"Race_Other\",\"Race_White\"],\n",
        "               \"Diabetic\":[\"Diabetic_No\",\"Diabetic_No, borderline diabetes\",\"Diabetic_Yes\",\"Diabetic_Yes (during pregnancy)\"],\n",
        "               \"PhysicalActivity\":[\"PhysicalActivity_No\",\"PhysicalActivity_Yes\"],\n",
        "               \"GenHealth\":[\"GenHealth_Excellent\",\"GenHealth_Fair\",\"GenHealth_Good\",\"GenHealth_Poor\",\"GenHealth_Very good\"],\n",
        "               \"Asthma\":[\"Asthma_No\",\"Asthma_Yes\"],\n",
        "               \"KidneyDisease\":[\"KidneyDisease_No\",\"KidneyDisease_Yes\"],\n",
        "               \"SkinCancer\":[\"SkinCancer_No\",\"SkinCancer_Yes\"],\n",
        "               \"Stroke\":[\"Stroke_No\",\"Stroke_Yes\"],\n",
        "               \"BMI_cat\":[\"BMI_cat_0\",\"BMI_cat_1\",\"BMI_cat_2\",\"BMI_cat_3\",\"BMI_cat_4\",\"BMI_cat_5\"],\n",
        "               \"PhysicalHealth_cat\":[\"PhysicalHealth_cat_0\",\"PhysicalHealth_cat_1\",\"PhysicalHealth_cat_2\",\"PhysicalHealth_cat_3\"],\n",
        "               \"MentalHealth_cat\":[\"MentalHealth_cat_0\",\"MentalHealth_cat_1\",\"MentalHealth_cat_2\",\"MentalHealth_cat_3\"],\n",
        "               \"AgeCategory_cat\":[\"AgeCategory_cat_0\",\"AgeCategory_cat_1\",\"AgeCategory_cat_2\",\"AgeCategory_cat_3\",\"AgeCategory_cat_4\",\"AgeCategory_cat_5\"],\n",
        "               \"SleepTime_cat\":[\"SleepTime_cat_0\",\"SleepTime_cat_1\",\"SleepTime_cat_2\",\"SleepTime_cat_3\",\"SleepTime_cat_4\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3MPgSFP4tri"
      },
      "outputs": [],
      "source": [
        "def entropy(x_col,df,y_col): #calculates the entropy of an occurrence --> First part of evaluation ; y_col: Name of the target variable\n",
        "  #filter dataframe by type of Diabetes for sum calculation in entropy function\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  df_grby=df.groupby(y_col)\n",
        "\n",
        "  a = df[x_col].to_numpy()\n",
        "  sum_012 = np.sum(a)  #calculate sum of y for hole dataframe\n",
        "\n",
        "  try:\n",
        "    df0=df_grby.get_group(0)\n",
        "    b = df0[x_col].to_numpy()\n",
        "    sum_0 = np.sum(b)    #calculate sum of filtered dataframe by just diabetes = 0\n",
        "    var_0=(sum_0/sum_012)  #calculate terms for entropy\n",
        "    term_0 = var_0*np.log2(var_0)  #calculate term for entropy\n",
        "    \n",
        "  except:             #if the grouped by dataframe is empty set var_0 to 0, because the log2(0) raises an error\n",
        "    term_0 = 0\n",
        "\n",
        "  try:\n",
        "    df1=df_grby.get_group(1)\n",
        "    c = df1[x_col].to_numpy()\n",
        "    sum_1 = np.sum(c)    #...\n",
        "    var_1=(sum_1/sum_012)\n",
        "    term_1 = var_1*np.log2(var_1)\n",
        "    \n",
        "  except:\n",
        "    term_1 = 0 \n",
        "\n",
        "  try:\n",
        "    df2=df_grby.get_group(2)   \n",
        "    d = df2[x_col].to_numpy()\n",
        "    sum_2 = np.sum(d)\n",
        "    var_2=(sum_2/sum_012)\n",
        "    term_2 = var_2*np.log2(var_2)\n",
        "    \n",
        "\n",
        "  except:\n",
        "    term_2 = 0\n",
        "\n",
        "  if math.isnan(term_0):\n",
        "    term_0 = 0 \n",
        "  if math.isnan(term_1):\n",
        "    term_1 = 0 \n",
        "  if math.isnan(term_2):\n",
        "    term_2 = 0 \n",
        "  #calculate entropy for every occurance\n",
        "  #entropy = -var_0*np.log2(var_0)-var_1*np.log2(var_1)-var_2*np.log2(var_2)\n",
        "  entropy = (-term_0 - term_1 - term_2)\n",
        "\n",
        "\n",
        "  return(sum_012,entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pCcfOUgIXT"
      },
      "source": [
        "# 3. Update new dataframe after split by multiplication of the probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LPCgpa4f7L_"
      },
      "outputs": [],
      "source": [
        "def update_df_by_split(split_name,c_dict,df,y_col): #calculate new dataframe for every split attribute - get as input the prior df and the split column as string\n",
        "    \n",
        "  #delete all splits to the coresponding attribute from the new dataframe\n",
        "  key_column=\"\"\n",
        "  for key in c_dict:\n",
        "    for x_col in c_dict[key]:\n",
        "      if x_col == split_name:\n",
        "        key_column = key \n",
        "\n",
        "\n",
        "  value_ary = c_dict[key_column]\n",
        "  #print(value_ary)\n",
        "  #print(key_column)\n",
        "\n",
        "  \n",
        "  \n",
        "  y=df[y_col]\n",
        "  df1=df.drop(y_col,axis=1)\n",
        "  split_ary = df[split_name]\n",
        "  df_new = df1 * split_ary[:,np.newaxis]\n",
        "  df_new[y_col]=y\n",
        "\n",
        "  for i in value_ary:\n",
        "    df_new.drop(i,axis=1,inplace=True)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  column_dict=c_dict.copy()\n",
        "  column_dict.pop(key_column, None)  \n",
        "  \n",
        "  \n",
        "  #value_ary = column_dict[key_column]\n",
        "  #print(value_ary)\n",
        "\n",
        "  return df_new.loc[(df_new.drop(y_col, axis=1)!=0).any(axis=1)],column_dict\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC_a6ibDMOZ8"
      },
      "source": [
        "# 4. Information-gain for every attribute and split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7tpuYeEGleB"
      },
      "outputs": [],
      "source": [
        "def info_min_attribute(c_dict,df,y_col):  #sum of the weighted entropies of the individual characteristics of an attribute\n",
        "  info_min=0\n",
        "  key_min =\"\"\n",
        "  for key in c_dict:\n",
        "    info_value=0\n",
        "    sum_valcounts = len(df)\n",
        "    for x_col in c_dict[key]: #iterate through every column key (Smoking, DiffWalking, BMI_cat...)\n",
        "      #print(x_col)\n",
        "      sum_012,ent=entropy(x_col,df,y_col)\n",
        "      #print(sum_012)\n",
        "      #print(ent)\n",
        "      info_value+=((sum_012/sum_valcounts)*ent)\n",
        "    #print(\"For the \",key,\" the information value is: \",info_value)\n",
        "    if info_min == 0:\n",
        "      info_min = info_value\n",
        "      key_min = key\n",
        "    elif info_value < info_min:\n",
        "      info_min = info_value\n",
        "      key_min = key\n",
        "\n",
        "\n",
        "  return (key_min,info_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxsfXsginz87"
      },
      "outputs": [],
      "source": [
        "def info_gain(ent_prior,ent_current): #calculate the infogain for every step\n",
        "  return ent_prior-ent_current"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFUcnvpmHH8i"
      },
      "outputs": [],
      "source": [
        "def info_min_split(info_min_att,c_dict,df,y_col): \n",
        "  #takes as input the name of the attribute with the minimal entropy, calcualte the entropy of every split attribute of the given Attribute with the smallest entropy\n",
        "  #result is a dataframe with the information status of every split\n",
        "  split_ary = []\n",
        "  for x_col in c_dict[info_min_att]:\n",
        "    sum_012,ent=entropy(x_col,df,y_col)\n",
        "    new_df, new_dict = update_df_by_split(x_col,c_dict,df,y_col)\n",
        "\n",
        "\n",
        "    split_ary.append([x_col,new_df,new_dict])\n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "  return split_ary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-UPEDaDNjV-"
      },
      "outputs": [],
      "source": [
        "def split_prediction (df,y_col): #calculate the class probabilities for each split\n",
        "\n",
        "  df_grby=df.groupby(y_col)\n",
        "  try:\n",
        "    df0 = df_grby.get_group(0)\n",
        "    y_0 = len(df0) / len(df)\n",
        "  except:\n",
        "    y_0 = 0\n",
        "  try:\n",
        "    df1 = df_grby.get_group(1)\n",
        "    y_1 = len(df1) / len(df)\n",
        "  except:\n",
        "    y_1 = 0\n",
        "  try:\n",
        "    df2 = df_grby.get_group(2)\n",
        "    y_2 = len(df2) / len(df)\n",
        "  except:\n",
        "    y_2 = 0\n",
        "  prediction = [float(y_0), float(y_1), float(y_2)]\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAPsPV72relp"
      },
      "source": [
        "# 5. Train the probabilistic decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6xcxI06uoHh"
      },
      "outputs": [],
      "source": [
        "def train_tree(c_dict,df_train,min_info_gain,max_depth,y_col): #train the tree by given df and dictionary\n",
        "\n",
        "  tree_ary = []\n",
        "  min_att0 = info_min_attribute(c_dict,df_train,y_col)  #returns Attribute with smales entropy, as well as the coresponding entropy of the attribute\n",
        "  split0 = info_min_split(min_att0[0],c_dict,df_train,y_col)  #returns occurences of the attribute with the samlest entropy incl. new_df and new dict\n",
        "  counter_lvl0=0\n",
        "  depth_counter = 0\n",
        "  for a in tqdm(split0):  #[GenHlth=0,GenHlth=1....]\n",
        "    split_name0 = a[0]  #GenHlt=0\n",
        "    new_df0 = a[1]\n",
        "    new_dict0 = a[2]\n",
        "    counter_lvl1 = 0\n",
        "    stop_counter1 = 0\n",
        "    \n",
        "\n",
        "    min_att1 = info_min_attribute(new_dict0,new_df0,y_col)  #HighBP\n",
        "    split1 = info_min_split(min_att1[0],new_dict0,new_df0,y_col)  #[HighBP=1, HighBP=0]\n",
        "     \n",
        "    tree_ary.append([split_name0, []])\n",
        "      \n",
        "   \n",
        "    for b in split1:\n",
        "      split_name1 = b[0] #HighBP=1\n",
        "      new_df1 = b[1]\n",
        "      new_dict1 = b[2]\n",
        "      counter_lvl2 = 0\n",
        "      stop_counter2 = 0\n",
        "      depth_counter = 2 \n",
        "    \n",
        "      min_att2 = info_min_attribute(new_dict1,new_df1,y_col)\n",
        "      split2 = info_min_split(min_att2[0],new_dict1,new_df1,y_col) \n",
        "      info_gain_1_2 = min_att0[1] - min_att1[1]\n",
        "\n",
        "  \n",
        "      if info_gain_1_2 >= min_info_gain and depth_counter<= max_depth:\n",
        "        tree_ary[counter_lvl0][1].append([split_name1, []])\n",
        "         \n",
        "        \n",
        "      elif stop_counter1 == 0:\n",
        "        tree_ary[counter_lvl0][1].append([split_prediction(new_df0,y_col)]) \n",
        "        stop_counter1+=1 \n",
        "        continue\n",
        "      else:\n",
        "        continue\n",
        "        \n",
        "      for c in split2:\n",
        "        split_name2 = c[0] \n",
        "        new_df2 = c[1]\n",
        "        new_dict2 = c[2] \n",
        "        counter_lvl3=0\n",
        "        stop_counter3 = 0\n",
        "        depth_counter = 3 \n",
        "\n",
        "        min_att3 = info_min_attribute(new_dict2,new_df2,y_col)\n",
        "        split3 = info_min_split(min_att3[0],new_dict2,new_df2,y_col)\n",
        "        info_gain_2_3 = min_att1[1] - min_att2[1]\n",
        "        \n",
        "        if info_gain_2_3 >= min_info_gain and depth_counter<= max_depth:\n",
        "          tree_ary[counter_lvl0][1][counter_lvl1][1].append([split_name2, []])\n",
        "          \n",
        "        \n",
        "        elif stop_counter2 == 0:\n",
        "          tree_ary[counter_lvl0][1][counter_lvl1][1].append([split_prediction(new_df1,y_col)]) \n",
        "          stop_counter2+=1 \n",
        "          continue\n",
        "        else:\n",
        "          continue       \n",
        "         \n",
        "        for d in split3:\n",
        "          split_name3 = d[0] \n",
        "          new_df3= d[1]\n",
        "          new_dict3 = d[2]\n",
        "          counter_lvl4 = 0\n",
        "          stop_counter4 = 0\n",
        "          depth_counter = 4 \n",
        "\n",
        "          min_att4 = info_min_attribute(new_dict3,new_df3,y_col)\n",
        "          split4 = info_min_split(min_att4[0],new_dict3,new_df3,y_col)\n",
        "          info_gain_3_4 = min_att2[1] - min_att3[1]\n",
        "       \n",
        "          if info_gain_3_4 >= min_info_gain and depth_counter<= max_depth:\n",
        "            tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1].append([split_name3, []])\n",
        "            \n",
        "        \n",
        "          elif stop_counter3 == 0:\n",
        "            tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1].append([split_prediction(new_df2,y_col)])\n",
        "            stop_counter3+=1 \n",
        "            continue \n",
        "          else:\n",
        "            continue\n",
        "\n",
        "          for e in split4:\n",
        "            split_name4 = e[0] \n",
        "            new_df4= e[1]\n",
        "            new_dict4 = e[2]\n",
        "            counter_lvl5 = 0\n",
        "            stop_counter5 = 0\n",
        "            depth_counter = 5 \n",
        "\n",
        "            min_att5 = info_min_attribute(new_dict4,new_df4,y_col)\n",
        "            split5 = info_min_split(min_att5[0],new_dict4,new_df4,y_col)\n",
        "            info_gain_4_5 = min_att3[1] - min_att4[1]\n",
        "         \n",
        "            if info_gain_4_5 >= min_info_gain and depth_counter<= max_depth:\n",
        "              tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1].append([split_name4, []])\n",
        "            \n",
        "        \n",
        "            elif stop_counter4 == 0:\n",
        "              tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1].append([split_prediction(new_df3,y_col)])\n",
        "              stop_counter4+=1 \n",
        "              continue \n",
        "            else:\n",
        "              continue\n",
        "\n",
        "            for f in split5:\n",
        "              split_name5 = f[0] \n",
        "              new_df5= f[1]\n",
        "              new_dict5 = f[2]\n",
        "              counter_lvl6 = 0\n",
        "              stop_counter6 = 0\n",
        "              depth_counter = 6 \n",
        "\n",
        "              min_att6 = info_min_attribute(new_dict5,new_df5,y_col)\n",
        "              split6 = info_min_split(min_att6[0],new_dict5,new_df5,y_col)\n",
        "              info_gain_5_6 = min_att4[1] - min_att5[1]\n",
        "              \n",
        "              if info_gain_5_6 >= min_info_gain and depth_counter<= max_depth:\n",
        "                tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1].append([split_name5, []])\n",
        "            \n",
        "        \n",
        "              elif stop_counter5 == 0:\n",
        "                tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1].append([split_prediction(new_df4,y_col)])\n",
        "                stop_counter5+=1 \n",
        "                continue \n",
        "              else:\n",
        "                continue\n",
        "\n",
        "              for g in split6:\n",
        "                split_name6 = g[0] \n",
        "                new_df6= g[1]\n",
        "                new_dict6 = g[2]\n",
        "                counter_lvl7 = 0\n",
        "                stop_counter7 = 0\n",
        "                depth_counter = 7\n",
        "\n",
        "                min_att7 = info_min_attribute(new_dict6,new_df6,y_col)\n",
        "                split7 = info_min_split(min_att7[0],new_dict6,new_df6,y_col)\n",
        "                info_gain_6_7 = min_att5[1] - min_att6[1]\n",
        "\n",
        "                if info_gain_6_7 >= min_info_gain and depth_counter<= max_depth:\n",
        "                  tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1].append([split_name6, []])\n",
        "            \n",
        "        \n",
        "                elif stop_counter6 == 0:\n",
        "                  tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1].append([split_prediction(new_df5,y_col)])\n",
        "                  stop_counter6+=1 \n",
        "                  continue \n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "                for h in split7:\n",
        "                  split_name7 = h[0] \n",
        "                  new_df7= h[1]\n",
        "                  new_dict7 = h[2]\n",
        "                  counter_lvl8 = 0\n",
        "                  stop_counter8 = 0\n",
        "                  depth_counter = 8\n",
        "\n",
        "                  min_att8 = info_min_attribute(new_dict7,new_df7,y_col)\n",
        "                  split8 = info_min_split(min_att8[0],new_dict7,new_df7,y_col)\n",
        "                  info_gain_7_8 = min_att6[1] - min_att7[1]\n",
        "\n",
        "                  if info_gain_7_8 >= min_info_gain and depth_counter<= max_depth:\n",
        "                    tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1][counter_lvl6][1].append([split_name7, []])\n",
        "            \n",
        "        \n",
        "                  elif stop_counter7 == 0:\n",
        "                    tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1][counter_lvl6][1].append([split_prediction(new_df6,y_col)])\n",
        "                    stop_counter7+=1 \n",
        "                    continue \n",
        "                  else:\n",
        "                    continue\n",
        "\n",
        "                  for i in split8:\n",
        "                    split_name8 = i[0] \n",
        "                    new_df8= i[1]\n",
        "                    new_dict8 = i[2]\n",
        "                    counter_lvl9 = 0\n",
        "                    stop_counter9 = 0\n",
        "                    depth_counter = 9\n",
        "\n",
        "                    min_att9 = info_min_attribute(new_dict8,new_df8,y_col)\n",
        "                    split9 = info_min_split(min_att9[0],new_dict8,new_df8,y_col)\n",
        "                    info_gain_8_9 = min_att7[1] - min_att8[1]\n",
        "\n",
        "                    if info_gain_8_9 >= min_info_gain and depth_counter<= max_depth:\n",
        "                      tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1][counter_lvl6][1][counter_lvl7][1].append([split_name8, []])\n",
        "            \n",
        "        \n",
        "                    elif stop_counter8 == 0:\n",
        "                      tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1][counter_lvl6][1][counter_lvl7][1].append([split_prediction(new_df7,y_col)])\n",
        "                      stop_counter8+=1 \n",
        "                      continue \n",
        "                    else:\n",
        "                      continue\n",
        "\n",
        "\n",
        "                    for j in split9:\n",
        "                      split_name9 = j[0] \n",
        "                      new_df9= j[1]\n",
        "                      new_dict9 = j[2]\n",
        "                      counter_lvl10= 0\n",
        "                      stop_counter10 = 0\n",
        "                      depth_counter = 10\n",
        "\n",
        "                      min_att10 = info_min_attribute(new_dict9,new_df9,y_col)\n",
        "                      split10 = info_min_split(min_att10[0],new_dict9,new_df9,y_col)\n",
        "                      info_gain_9_10 = min_att8[1] - min_att9[1]\n",
        "\n",
        "                      if info_gain_9_10 >= min_info_gain and depth_counter<= max_depth:\n",
        "                        tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1][counter_lvl6][1][counter_lvl7][1][counter_lvl8][1].append([split_name9, []])\n",
        "            \n",
        "        \n",
        "                      elif stop_counter9 == 0:\n",
        "                        tree_ary[counter_lvl0][1][counter_lvl1][1][counter_lvl2][1][counter_lvl3][1][counter_lvl4][1][counter_lvl5][1][counter_lvl6][1][counter_lvl7][1][counter_lvl8][1].append([split_prediction(new_df8,y_col)])\n",
        "                        stop_counter9+=1 \n",
        "                        continue \n",
        "                      else:\n",
        "                        continue\n",
        "\n",
        "\n",
        "                          \n",
        "                                     \n",
        "                        counter_lvl10+=1 \n",
        "                      counter_lvl9+=1 \n",
        "                    counter_lvl8+=1          \n",
        "                  counter_lvl7+=1\n",
        "                counter_lvl6+=1\n",
        "              counter_lvl5+=1\n",
        "            counter_lvl4+=1\n",
        "          counter_lvl3+=1\n",
        "        counter_lvl2+=1\n",
        "      counter_lvl1+=1\n",
        "    counter_lvl0+=1\n",
        "\n",
        "\n",
        "  \n",
        "  return tree_ary\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y38pqnvE--X"
      },
      "outputs": [],
      "source": [
        "#tree_ary=train_tree(column_dict,df,0.01,8,\"HeartDisease\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJEAZ_E_BBdc"
      },
      "outputs": [],
      "source": [
        "#tree_ary[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9rcCuICdzDx"
      },
      "source": [
        "# 6. Predict by using pretrained Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOOMi5xuiL3c"
      },
      "outputs": [],
      "source": [
        "def check_type(lvl_list): #check if the given list containts floats -->stop because end of tree reached\n",
        "  converted_list = np.array(lvl_list)\n",
        "  return converted_list.dtype == \"float64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDvgEikpp6BF"
      },
      "outputs": [],
      "source": [
        "def class_prob(lvl_list, multi): #multily final node prediction with multiplied path\n",
        "  converted_list = np.array(lvl_list)\n",
        "  return converted_list * multi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcU9VyMo17Mz"
      },
      "outputs": [],
      "source": [
        "def sum_class_prob(predict_ary): #calculate final prediction for one patient by iteration through all path prediction arrays\n",
        "  y_0 = 0\n",
        "  y_1 = 0\n",
        "  y_2 = 0\n",
        "  class_prob = []\n",
        "  for i in range(0, len(predict_ary)):\n",
        "    try:\n",
        "      y_0 += predict_ary[i][0]\n",
        "    except:\n",
        "      y_0 += 0\n",
        "    try:\n",
        "      y_1 += predict_ary[i][1]\n",
        "    except:\n",
        "      y_1 += 0\n",
        "    try:\n",
        "      y_2 += predict_ary[i][2]\n",
        "    except:\n",
        "      y_2 += 0\n",
        "  class_prob.append([y_0,y_1,y_2])\n",
        "  return class_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B167IzE9zMQX"
      },
      "outputs": [],
      "source": [
        "def predict(trained_tree, df): #predict Diabetes target value distribution for every instance by given dataframe and trained tree\n",
        "  \n",
        "  predictproba_diabetes_all_patients = []\n",
        "  for index, row in df.iterrows(): #get every instance step by step from the given test data and predict for it the probabilities \n",
        "    predict_ary = []\n",
        "    for split1 in range(0,len(trained_tree)):\n",
        "      lvl1=trained_tree[split1][0]\n",
        "      lvl1_value = row[lvl1]\n",
        "\n",
        "      for split2 in range(0,len(trained_tree[split1][1])):\n",
        "        lvl2=trained_tree[split1][1][split2][0] #HighBP =0\n",
        "        \n",
        "        if check_type(lvl2) == True:\n",
        "          predict_ary.append(class_prob(lvl2,lvl1_value))\n",
        "          continue\n",
        "        else:\n",
        "          lvl2_value = row[lvl2]\n",
        "          multi1_2 = lvl1_value * lvl2_value\n",
        "\n",
        "          for split3 in range(0,len(trained_tree[split1][1][split2][1])):\n",
        "              lvl3=trained_tree[split1][1][split2][1][split3][0] #Age=4\n",
        "\n",
        "              if check_type(lvl3) == True:\n",
        "                predict_ary.append(class_prob(lvl3,multi1_2))\n",
        "\n",
        "                continue\n",
        "              else:\n",
        "                lvl3_value = row[lvl3]\n",
        "                multi2_3 = lvl3_value * multi1_2\n",
        "\n",
        "                for split4 in range(0,len(trained_tree[split1][1][split2][1][split3][1])):\n",
        "                  lvl4=trained_tree[split1][1][split2][1][split3][1][split4][0]  \n",
        "                  if check_type(lvl4) == True:\n",
        "                    predict_ary.append(class_prob(lvl4,multi2_3))\n",
        "\n",
        "                    continue\n",
        "                  else:\n",
        "                    lvl4_value = row[lvl4]\n",
        "                    multi3_4 = lvl4_value * multi2_3\n",
        "                  \n",
        "                    for split5 in range(0,len(trained_tree[split1][1][split2][1][split3][1][split4][1])):\n",
        "                      lvl5=trained_tree[split1][1][split2][1][split3][1][split4][1][split5][0]  \n",
        "                      if check_type(lvl5) == True:\n",
        "                        predict_ary.append(class_prob(lvl5,multi3_4))\n",
        "\n",
        "                        continue\n",
        "                      else:\n",
        "                        lvl5_value = row[lvl5]\n",
        "                        multi4_5 = lvl5_value * multi3_4\n",
        "                      \n",
        "                        for split6 in range(0,len(trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1])):\n",
        "                          lvl6=trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][0]  \n",
        "                          if check_type(lvl6) == True:\n",
        "                            predict_ary.append(class_prob(lvl6,multi4_5))\n",
        "\n",
        "                            continue\n",
        "                          else:\n",
        "                            lvl6_value = row[lvl6]\n",
        "                            multi5_6 = lvl6_value * multi4_5\n",
        "\n",
        "                            for split7 in range(0,len(trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1])):\n",
        "                              lvl7=trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1][split7][0]   \n",
        "                              if check_type(lvl7) == True:\n",
        "                                predict_ary.append(class_prob(lvl7,multi5_6))\n",
        "\n",
        "                                continue\n",
        "                              else:\n",
        "                                lvl7_value = row[lvl7]\n",
        "                                multi6_7 = lvl7_value * multi5_6\n",
        "\n",
        "                                for split8 in range(0,len(trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1][split7][1])):\n",
        "                                  lvl8=trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1][split7][1][split8][0]    \n",
        "                                  if check_type(lvl8) == True:\n",
        "                                    predict_ary.append(class_prob(lvl8,multi6_7))\n",
        "\n",
        "                                    continue\n",
        "                                  else:\n",
        "                                    lvl8_value = row[lvl8]\n",
        "                                    multi7_8 = lvl8_value * multi6_7 \n",
        "\n",
        "                                    for split9 in range(0,len(trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1][split7][1][split8][1])):\n",
        "                                      lvl9=trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1][split7][1][split8][1][split9][0]   \n",
        "                                      if check_type(lvl9) == True:\n",
        "                                        predict_ary.append(class_prob(lvl9,multi7_8))\n",
        "\n",
        "                                        continue\n",
        "                                      else:\n",
        "                                        lvl9_value = row[lvl9]\n",
        "                                        multi8_9 = lvl9_value * multi7_8 \n",
        "\n",
        "                                        for split10 in range(0,len(trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1][split7][1][split8][1][split9][1])):\n",
        "                                          lvl10=trained_tree[split1][1][split2][1][split3][1][split4][1][split5][1][split6][1][split7][1][split8][1][split9][1][split10][0]   \n",
        "                                          if check_type(lvl10) == True:\n",
        "                                            predict_ary.append(class_prob(lvl10,multi8_9))\n",
        "\n",
        "                                            continue\n",
        "                                          else:\n",
        "                                            lvl10_value = row[lvl10]\n",
        "                                            multi9_10 = lvl10_value * multi8_9 \n",
        "\n",
        "\n",
        "    predictproba_diabetes_all_patients.append(sum_class_prob(predict_ary))\n",
        "  return predictproba_diabetes_all_patients \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZvokQC-fl-t"
      },
      "outputs": [],
      "source": [
        "#b = predict(tree_ary,df[:600])\n",
        "#b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr2KwieEyHBu"
      },
      "source": [
        "# 7. Probabilistic Random Forest Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS_x2ADN5lC4"
      },
      "outputs": [],
      "source": [
        "def bootstrapping(c_dict,df,validation_size,n_features): #give column dictonary and df as well as the number of instances and features after bagging\n",
        "  df_bt_train = df.copy() #bt: Bootstrapped for training the trees\n",
        "  c_dict_bt = c_dict.copy()\n",
        "\n",
        "  df_bt_train = df_bt_train[0:0]\n",
        "  df_bt_validation = df_bt_train.copy() #bt: All rows which are not in train sample are in validation sample for MSE Calucaltion of the tree\n",
        "  \n",
        "  n_instances = int(round(len(df) * (1 -  validation_size),0))\n",
        "  df_bt_train = df.sample(n = n_instances, replace = True) #pick randomly given number of instances from dataframe with replacement\n",
        "\n",
        "  random_features = []\n",
        "  for key, value in sorted(c_dict.items(), key=lambda x: random.random()): #randomly pick keys(columns/features) from column_dict\n",
        "    random_features.append(key)\n",
        "  \n",
        "  random_features = random_features[0:(len(random_features)-n_features)]\n",
        "  #print(random_features) see the selected number of features which will be deleted ['PhysHlth', 'HighChol', 'AnyHealthcare',\n",
        "  delete_occurrences = []\n",
        "  for occurrences in random_features:\n",
        "    delete_occurrences.extend(c_dict[occurrences])\n",
        "\n",
        "  #print(delete_occurrences) see the occurrences coresponding to the features ['PhysHlth_cat=1', 'PhysHlth_cat=0', 'HighChol=1', 'HighChol=0',...\n",
        "\n",
        "  for occurrences in delete_occurrences:  #delete the chosen occurrences from the dataframe\n",
        "    df_bt_train.drop(columns=occurrences, axis=1,inplace = True)\n",
        "  \n",
        "  for feature in random_features:\n",
        "    c_dict_bt.pop(feature)\n",
        "\n",
        "  df_bt_validation = df.drop(df_bt_train.index) #get all the rows which are not in the bootstrapped train dataframe\n",
        "\n",
        "  return df_bt_train,df_bt_validation, c_dict_bt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQsJThcqAGw9"
      },
      "outputs": [],
      "source": [
        "def y_encoding(df,y_col): #one hot encoding of true y of every instance\n",
        "  y_total_df = []\n",
        "  for index, row in df.iterrows():\n",
        "    y_instance = [0,0,0]\n",
        "    if row[y_col] == 0:\n",
        "      y_instance[0] = 1\n",
        "    elif row[y_col] == 1:\n",
        "      y_instance[1] = 1\n",
        "    elif row[y_col] == 2:\n",
        "      y_instance[2] = 1\n",
        "    y_total_df.append(y_instance)\n",
        "  return y_total_df #return the hole df the one hot encoding in one array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqFEVSaVQK6N"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error_tree(df,y_encoding,prediction): #calcualte mean squared error for prediction array of tree and the one hot encoding array of real value y\n",
        "  mse_total = 0\n",
        "  for i in range(0,len(df)):\n",
        "\n",
        "    mse_total += (y_encoding[i][0] - prediction[i][0][0])**2 + (y_encoding[i][1] - prediction[i][0][1])**2 + (y_encoding[i][2] - prediction[i][0][2])**2\n",
        " \n",
        "  return (mse_total / len(df))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Frw_YCEbAaLi"
      },
      "outputs": [],
      "source": [
        "def random_forest_fit(c_dict,df_train,y_col, n_trees,n_trees_final, min_info_gain,max_depth,validation_size, n_features,methode):\n",
        "\n",
        "  my_dict = {}                       #safes every tree as key with its coresponding value as value\n",
        "  my_dict_key = {}  \n",
        "  n_trees_final_ary = []             #safe n (n_trees_final) number of best trees as trained trees and return it\n",
        "  counter = 0\n",
        "  for tree in tqdm(range(0,n_trees)): #create n_trees number of trees \n",
        "    #for every tree do a bootstrapping of rows and columns, split given train dataframe (df) into train and validation sample\n",
        "    df_bt_train,df_bt_validation, c_dict_bt = bootstrapping(c_dict,df_train,validation_size,n_features) \n",
        "    y_encode_train = y_encoding(df_bt_train,y_col)\n",
        "    y_encode_validation = y_encoding(df_bt_validation,y_col)\n",
        "    #train every tree based on the bootstrapping\n",
        "    trained_tree = train_tree(c_dict_bt, df_bt_train, min_info_gain,max_depth,y_col)\n",
        "    #make a prediction for every instance based on the trained tree\n",
        "    prediction_train = predict(trained_tree, df_bt_train)\n",
        "    prediction_test = predict(trained_tree, df_bt_validation)\n",
        "\n",
        "    if methode == \"mse\":\n",
        "     #calculate mean squared error for the given tree for train and test data\n",
        "     train_mse = mean_squared_error_tree(df_bt_train, y_encode_train, prediction_train)\n",
        "     validation_mse = mean_squared_error_tree(df_bt_validation, y_encode_validation, prediction_test) #get mse fr validation data sete\n",
        "\n",
        "     my_dict_key.update({counter: trained_tree})\n",
        "     my_dict.update({counter: validation_mse})\n",
        "     my_dict_key.update({counter: trained_tree})\n",
        "     my_dict.update({counter: validation_mse})\n",
        "     counter+=1\n",
        "\n",
        "    elif methode == \"accuracy\":\n",
        "      #calculate accuracy and take it choosing the trees\n",
        "      y_true_val = df_bt_validation[\"HeartDisease\"]\n",
        "      y_pred_val = convert_prob_to_class(prediction_test)\n",
        "      y_true_train = df_bt_train[\"HeartDisease\"]\n",
        "      y_pred_train = convert_prob_to_class(prediction_train)\n",
        "      accuracy_val = accuracy_score(y_true_val, y_pred_val)\n",
        "      accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
        "      print(\"Train Accuracy: \",accuracy_train)\n",
        "      print(\"Validation Accuracy: \",accuracy_val) \n",
        "      my_dict_key.update({counter: trained_tree})\n",
        "      my_dict.update({counter: accuracy_val})\n",
        "      counter+=1\n",
        "      \n",
        "\n",
        "\n",
        "    \n",
        "  if methode == \"mse\":\n",
        "   # Finding n = n_trees_final number of smalest mse\n",
        "   get_counter_keys = nsmallest(n_trees_final, my_dict, key = my_dict.get)\n",
        "   for i in get_counter_keys:\n",
        "     n_trees_final_ary.append(my_dict_key[i])\n",
        "  elif methode == \"accuracy\":\n",
        "   get_counter_keys = sorted(range(len(my_dict)), key = lambda sub: my_dict[sub])[-n_trees_final:]\n",
        "   for i in get_counter_keys:\n",
        "     n_trees_final_ary.append(my_dict_key[i])\n",
        "\n",
        "\n",
        "\n",
        "  return n_trees_final_ary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOhsDi4eDfbM"
      },
      "outputs": [],
      "source": [
        "#n_trees_final_ary = random_forest_fit(c_dict= column_dict,df_train= df,y_col =\"HeartDisease\",n_trees= 10,n_trees_final= 3, min_info_gain= 0.01,max_depth=6,validation_size = 0.25, n_features=15,methode=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKV-d-qxPjNo"
      },
      "outputs": [],
      "source": [
        "def random_forest_classifier(n_trees_final_ary, df_test, c_dict):    #predicts classes/y based on the best trees from the Random Forrest\n",
        " safe_prediction = []\n",
        " final_prediction = []\n",
        " for tree in n_trees_final_ary:\n",
        "   prediction_test = predict(tree, df_test)\n",
        "   safe_prediction.append(prediction_test)\n",
        " \n",
        "\n",
        " for y_pred_j in range(0,len(df_test)):\n",
        "    y_0 = 0\n",
        "    y_1 = 0\n",
        "    y_2 = 0\n",
        "\n",
        "    for y_pred_i in range(0,len(safe_prediction)): #iterate for one instance trough every prediction from the given trees and take the mean\n",
        "      y_0 += safe_prediction[ y_pred_i][y_pred_j][0][0]\n",
        "      y_1 += safe_prediction[ y_pred_i][y_pred_j][0][1]\n",
        "      y_2 += safe_prediction[ y_pred_i][y_pred_j][0][2] \n",
        "\n",
        "    y_0_mean = y_0/len(safe_prediction)\n",
        "    y_1_mean = y_1/len(safe_prediction)\n",
        "    y_2_mean = y_2/len(safe_prediction)\n",
        "    final_prediction.append([y_0_mean, y_1_mean, y_2_mean])\n",
        "\n",
        "   \n",
        " return final_prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7iolnf72wqt"
      },
      "outputs": [],
      "source": [
        "#final_prediction = random_forest_classifier(n_trees_final_ary,df[:10],column_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRbRWBlnK5m_"
      },
      "outputs": [],
      "source": [
        "#final_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ofJW7CZdqE"
      },
      "source": [
        "# Example Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_np0Ne5onvV"
      },
      "outputs": [],
      "source": [
        "def convert_prob_to_class(final_prediction): \n",
        " #converte the probability array final prediction into categorial data based on highest probability\n",
        "  final_prediction_class = []\n",
        "  for i in final_prediction:\n",
        "    i = np.array(i)\n",
        "    maxindex = i.argmax()\n",
        "    final_prediction_class.append(maxindex)\n",
        "\n",
        "  return final_prediction_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXSrRdV4Zf-b",
        "outputId": "1a2951c0-04cb-4268-8828-bd20ccb5417e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:05<00:25,  5.10s/it]\u001b[A\n",
            " 33%|      | 2/6 [00:08<00:17,  4.31s/it]\u001b[A\n",
            " 50%|     | 3/6 [00:14<00:14,  4.85s/it]\u001b[A\n",
            " 67%|   | 4/6 [00:19<00:10,  5.03s/it]\u001b[A\n",
            " 83%| | 5/6 [00:23<00:04,  4.50s/it]\u001b[A\n",
            "100%|| 6/6 [00:25<00:00,  4.29s/it]\n",
            " 20%|        | 1/5 [00:28<01:54, 28.59s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:08<00:41,  8.23s/it]\u001b[A\n",
            " 33%|      | 2/6 [00:17<00:34,  8.69s/it]\u001b[A\n",
            " 50%|     | 3/6 [00:35<00:38, 12.92s/it]\u001b[A\n",
            " 67%|   | 4/6 [00:46<00:24, 12.41s/it]\u001b[A\n",
            " 83%| | 5/6 [00:51<00:09,  9.61s/it]\u001b[A\n",
            "100%|| 6/6 [00:52<00:00,  8.77s/it]\n",
            " 40%|      | 2/5 [01:25<02:15, 45.11s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:04<00:22,  4.58s/it]\u001b[A\n",
            " 33%|      | 2/6 [00:13<00:29,  7.27s/it]\u001b[A\n",
            " 50%|     | 3/6 [00:27<00:31, 10.44s/it]\u001b[A\n",
            " 67%|   | 4/6 [00:37<00:20, 10.13s/it]\u001b[A\n",
            " 83%| | 5/6 [00:50<00:11, 11.20s/it]\u001b[A\n",
            "100%|| 6/6 [00:51<00:00,  8.55s/it]\n",
            " 60%|    | 3/5 [02:20<01:39, 49.81s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:03<00:18,  3.75s/it]\u001b[A\n",
            " 33%|      | 2/6 [00:07<00:14,  3.57s/it]\u001b[A\n",
            " 50%|     | 3/6 [00:13<00:14,  4.95s/it]\u001b[A\n",
            " 67%|   | 4/6 [00:19<00:10,  5.12s/it]\u001b[A\n",
            " 83%| | 5/6 [00:25<00:05,  5.60s/it]\u001b[A\n",
            "100%|| 6/6 [00:29<00:00,  4.89s/it]\n",
            " 80%|  | 4/5 [02:51<00:42, 42.47s/it]\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:15<01:19, 15.93s/it]\u001b[A\n",
            " 33%|      | 2/6 [00:24<00:46, 11.74s/it]\u001b[A\n",
            " 50%|     | 3/6 [00:42<00:43, 14.60s/it]\u001b[A\n",
            " 67%|   | 4/6 [00:52<00:25, 12.61s/it]\u001b[A\n",
            " 83%| | 5/6 [01:00<00:10, 10.97s/it]\u001b[A\n",
            "100%|| 6/6 [01:04<00:00, 10.75s/it]\n",
            "100%|| 5/5 [03:58<00:00, 47.77s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dftrain = df_splitknn[:600]\n",
        "dftest = df_splitknn[600:]\n",
        "\n",
        "#fit the random forest\n",
        "n_trees_final_ary = random_forest_fit(c_dict= column_dict, df_train = dftrain, y_col =\"HeartDisease\",n_trees = 5,\n",
        "                                      n_trees_final = 3, min_info_gain = 0.1,max_depth = 4,validation_size = 0.2, n_features = 14, methode = \"mse\")\n",
        "\n",
        "#predict with the random forest based on the test data set\n",
        "final_prediction = random_forest_classifier(n_trees_final_ary, dftest, column_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dftest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "2QW5OFYTpksr",
        "outputId": "5a453c78-2407-4c19-a11b-176c5cfd75ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (59) exceeds max_columns (20). Falling back to pandas display.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     HeartDisease  Smoking_No  Smoking_Yes  AlcoholDrinking_No  \\\n",
              "600           NaN       1.000        0.000            0.818182   \n",
              "601           NaN       0.625        0.375            1.000000   \n",
              "602           NaN       1.000        0.000            0.800000   \n",
              "\n",
              "     AlcoholDrinking_Yes  DiffWalking_No  DiffWalking_Yes  Sex_Female  \\\n",
              "600             0.181818           1.000            0.000         0.0   \n",
              "601             0.000000           0.875            0.125         1.0   \n",
              "602             0.200000           1.000            0.000         0.0   \n",
              "\n",
              "     Sex_Male  Race_American Indian/Alaskan Native  ...  AgeCategory_cat_1  \\\n",
              "600       1.0                                  0.0  ...           0.000000   \n",
              "601       0.0                                  0.0  ...           0.000000   \n",
              "602       1.0                                  0.0  ...           0.111111   \n",
              "\n",
              "     AgeCategory_cat_2  AgeCategory_cat_3  AgeCategory_cat_4  \\\n",
              "600           0.000000           0.000000           0.000000   \n",
              "601           0.111111           0.444444           0.000000   \n",
              "602           0.333333           0.000000           0.111111   \n",
              "\n",
              "     AgeCategory_cat_5  SleepTime_cat_0  SleepTime_cat_1  SleepTime_cat_2  \\\n",
              "600                0.0         0.000000         1.000000             0.00   \n",
              "601                0.0         0.000000         0.300000             0.70   \n",
              "602                0.0         0.083333         0.666667             0.25   \n",
              "\n",
              "     SleepTime_cat_3  SleepTime_cat_4  \n",
              "600              0.0              0.0  \n",
              "601              0.0              0.0  \n",
              "602              0.0              0.0  \n",
              "\n",
              "[3 rows x 59 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-674653c5-18df-4271-bda8-889cbeab69b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeartDisease</th>\n",
              "      <th>Smoking_No</th>\n",
              "      <th>Smoking_Yes</th>\n",
              "      <th>AlcoholDrinking_No</th>\n",
              "      <th>AlcoholDrinking_Yes</th>\n",
              "      <th>DiffWalking_No</th>\n",
              "      <th>DiffWalking_Yes</th>\n",
              "      <th>Sex_Female</th>\n",
              "      <th>Sex_Male</th>\n",
              "      <th>Race_American Indian/Alaskan Native</th>\n",
              "      <th>...</th>\n",
              "      <th>AgeCategory_cat_1</th>\n",
              "      <th>AgeCategory_cat_2</th>\n",
              "      <th>AgeCategory_cat_3</th>\n",
              "      <th>AgeCategory_cat_4</th>\n",
              "      <th>AgeCategory_cat_5</th>\n",
              "      <th>SleepTime_cat_0</th>\n",
              "      <th>SleepTime_cat_1</th>\n",
              "      <th>SleepTime_cat_2</th>\n",
              "      <th>SleepTime_cat_3</th>\n",
              "      <th>SleepTime_cat_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>600</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows  59 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-674653c5-18df-4271-bda8-889cbeab69b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-674653c5-18df-4271-bda8-889cbeab69b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-674653c5-18df-4271-bda8-889cbeab69b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Thomas hat mit einer Wahrscheinlichkeit von \",(final_prediction[0][0]*100),\"% kein Herzerkrankung\")\n",
        "print(\"Alina hat mit einer Wahrscheinlichkeit von \",(final_prediction[1][0]*100),\"% kein Herzerkrankung\")\n",
        "print(\"Lucas hat mit einer Wahrscheinlichkeit von \",(final_prediction[2][0]*100),\"% kein Herzerkrankung\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyHOqgxol3Ht",
        "outputId": "afa77961-8a37-43b2-f7b8-2b42a6f5f7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thomas hat mit einer Wahrscheinlichkeit von  79.30615589690667 % kein Herzerkrankung\n",
            "Alina hat mit einer Wahrscheinlichkeit von  66.69711749395088 % kein Herzerkrankung\n",
            "Lucas hat mit einer Wahrscheinlichkeit von  63.4605051733621 % kein Herzerkrankung\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}